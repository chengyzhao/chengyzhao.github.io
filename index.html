<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chengyang Zhao</title>
  
  <meta name="author" content="Chengyang Zhao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="VFpDtaEgjJXaA29nStin_tiUcf_lFQeFPIPe7cDi04g" />
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üçä</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chengyang Zhao</name>
              </p>
              <p>
                I am a final-year undergraduate student at <a href="https://yuanpei.pku.edu.cn/en/">Yuanpei College</a>, <a href="https://english.pku.edu.cn">Peking University</a> in China, majoring in Data Science and Big Data Technology (Statistics + Computer Science). 
                I am fortunate to be advised by Prof. <a href="https://hughw19.github.io">He Wang</a>. I am also privileged to work closely with Prof. <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a> and Prof. <a href="https://yunzhuli.github.io">Yunzhu Li</a>.
              </p>
              <p>
                My research interest is broadly in <b>computer vision (especially 3D vision)</b>, <b>robotics</b>, and <b>multi-modal learning</b>. My research objective is to build an intelligent agent with comprehensive perception, reasoning, and execution capabilities developed from multi-modal information, which can interact effectively and efficiently with the real world.
              </p>
              <p style="text-align:center">
                <a href="mailto:zhaochengyang@pku.edu.cn">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV.pdf">CV (Dec. 2023)</a> &nbsp/&nbsp -->
                <a href="https://twitter.com/alecyzh">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/chengyzhao">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ChengyangZhao.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/GAPartNet_teaser.png" alt="GAPartNet" width="300" height="110" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
            <a href="https://pku-epic.github.io/GAPartNet/" id="GAPartNet">
            <papertitle>GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts</papertitle>
            </a>
            <br>
            <a href="https://geng-haoran.github.io"><author>Haoran Geng*</author></a>, 
            <a href="https://helinxu.github.io/"><author>Helin Xu*</author></a>,
            <strong>Chengyang Zhao*</strong>,
            <a href="https://chaoxu.xyz/#bio"><author>Chao Xu</author></a>,
            <a href="https://ericyi.github.io/"><author>Li Yi</author></a>,
            <a href="https://siyuanhuang.com/"><author>Siyuan Huang</author></a>,
            <a href="https://hughw19.github.io/"><author>He Wang</author></a>
            <br>
            (* Equal contribution with the order determined by rolling dice.)
            <br>
            <em>CVPR</em>, 2023, <b>Highlight</b> (top 2.5% of all submissions) with <b>all top ratings</b>
            <br>
            [<a href="https://arxiv.org/abs/2211.05272">paper</a>]
            [<a href="https://pku-epic.github.io/GAPartNet/">webpage</a>]
            [<a href="https://github.com/PKU-EPIC/GAPartNet">code</a>]
            [<a href="https://forms.gle/3qzv8z5vP2BT5ARN7">dataset</a>]
            <p>We propose to learn cross-category generalizable object perception and manipulation skills via Generalizable and Actionable Parts (GAParts). We present GAPartNet, a large-scale interactive dataset with rich, part-level annotations for both perception and interaction tasks.</p>
        </td>
        </tr>

        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/TextPSG_teaser.png" alt="TextPSG" width="300" height="172" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
            <a href="https://vis-www.cs.umass.edu/TextPSG/" id="TextPSG">
            <papertitle>TextPSG: Panoptic Scene Graph Generation from Textual Descriptions</papertitle>
            </a>
            <br>
            <strong>Chengyang Zhao</strong>,
            <a href="https://scholar.google.com/citations?user=qff5rRYAAAAJ&hl=en"><author>Yikang Shen</author></a>,
            <a href="https://zfchenunique.github.io/"><author>Zhenfang Chen</author></a>,
            <a href="https://dingmyu.github.io/"><author>Mingyu Ding</author></a>,
            <a href="https://people.csail.mit.edu/ganchuang"><author>Chuang Gan</author></a>
            <em>ICCV</em>, 2023
            <br>
            [<a href="https://arxiv.org/abs/2310.07056v1">paper</a>]
            [<a href="https://vis-www.cs.umass.edu/TextPSG/">webpage</a>]
            [<a href="https://github.com/chengyzhao/TextPSG">code</a>]
            <p>We introduce a novel problem aiming to learn panoptic scene graph generation entirely from textual descriptions. We design a modularized proposal-free framework, which not only breaks the generalization limitation within previous detector-based methods but also learns extensive and various object semantics and relation predicates from text.</p>
        </td>
        </tr>

        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/SNRF_teaser.png" alt="TextPSG" width="300" height="109" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
            <papertitle>Controllable 3D Scene Editing with Sparse Neural Radiance Fields</papertitle>
            </a>
            <br>
            <a href="https://robo-alex.github.io"><author>Mingtong Zhang*</author></a>,
            <strong>Chengyang Zhao*</strong>,
            <a href="https://evelinehong.github.io/"><author>Yining Hong</author></a>,
            <author>Hongsheng Lu</author>,
            <a href="https://people.csail.mit.edu/ganchuang"><author>Chuang Gan</author></a>
            <br>
            (* Equal contribution.)
            <br>
            <em>CVPR</em>, 2024, In Submission
            <br>
            <p>We propose a novel scene representation Sparse Neural Radiance Fields to compartmentalize the 3D scene into specialized expert fields, and design a DINO-based gating mechanism for automatic semantics-based scene decomposition across the expert fields during reconstruction. Our proposed representation effectively enhances controllability, enabling more precise and intuitive editing on 3D scenes.</p>
        </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src="images/UIUC_logo.png", width="100">
        </td>
        <td width="90%" valign="center">
            <strong>University of Illinois Urbana-Champaign</strong>
            <br>
            2023.06 - Present
            <br>
            <strong>Undergraduate Research Intern (Remote)</strong>
            <br> Research Advisor: Prof. <a href="https://yunzhuli.github.io">Yunzhu Li</a>
        </td>
        </tr>  

        <tr>
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src="images/MIT_logo.png", width="100">
        </td>
        <td width="90%" valign="center">
            <strong>Massachusetts Institute of Technology</strong>
            <br>
            2022.08 - 2023.05
            <br>
            <strong>Visiting Undergraduate Student</strong>
            <br> Research Advisor: Dr. <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a> 
        </td>
        </tr>  

        <tr>
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src="images/PKU_logo.png", width="95">
        </td>
        <td width="90%" valign="center">
            <strong>Peking University</strong>
            <br>
            2021.05 - Present
            <br>
            <strong>Undergraduate Research Intern</strong>
            <br> Research Advisor: Prof. <a href="https://hughw19.github.io">He Wang</a>
        </td>
        </tr>  

        <tr>
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src="images/PKU_logo.png", width="95">
        </td>
        <td width="90%" valign="center">
            <strong>Peking University</strong>
            <br>
            2019.09 - Present
            <br>
            <strong>Undergraduate Student</strong>, <a href="https://yuanpei.pku.edu.cn/en/">Yuanpei College</a>
            <br> GPA ranking: 2/13
        </td>
        </tr>  
        </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Awards and Honors</heading>
              <p>&bull; <strong>SenseTime Scholarship</strong> (<b>30</b> undergraduate students/year in China), SenseTime, 2023</p>
              <p>&bull; <strong>Third Prize of Peking University Scholarship</strong>, Peking University, 2022</p>
              <p>&bull; <strong>Award for Academic Excellence</strong>, Peking University, 2022</p>
              <p>&bull; <strong>Xiaomi Scholarship</strong>, Peking University, 2020</p>
              <p>&bull; <strong>Merit Student</strong>, Peking University, 2020</p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template :D
                <br>
                Last Updated: Dec. 30, 2023
              </p>
            </td>
          </tr>
        </tbody></table>

        
      </td>
    </tr>
  </table>
</body>

</html>
